{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlDldDO2CkUY"
      },
      "source": [
        "# CS 101 Final Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKEkIH0DCkUZ"
      },
      "source": [
        "## Part 1: Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0Xl3X8bCkUZ"
      },
      "source": [
        "### Dataset Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZwI7jszLCkUZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import cohen_kappa_score, f1_score\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import re\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "RnQN4Y1uCkUa"
      },
      "outputs": [],
      "source": [
        "seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "50uvFqmtCkUa"
      },
      "outputs": [],
      "source": [
        "data_er = pd.read_csv(\"https://raw.githubusercontent.com/behavioral-data/Empathy-Mental-Health/refs/heads/master/dataset/emotional-reactions-reddit.csv\", index_col=0)\n",
        "data_exp = pd.read_csv(\"https://raw.githubusercontent.com/behavioral-data/Empathy-Mental-Health/refs/heads/master/dataset/explorations-reddit.csv\", index_col=0)\n",
        "data_int = pd.read_csv(\"https://raw.githubusercontent.com/behavioral-data/Empathy-Mental-Health/refs/heads/master/dataset/interpretations-reddit.csv\", index_col=0)\n",
        "sampled_data = data_exp.sample(n=1000, random_state=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3jEly3zbCkUb"
      },
      "outputs": [],
      "source": [
        "#  keep the data that we are using for each model constant\n",
        "sampled_data.to_csv('sampled_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKPeMBTQCkUb",
        "outputId": "f7d9a4bb-954e-4122-ee6d-56972106295b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/g3/6zxrpl516gj47w81m5p9v8rc0000gn/T/ipykernel_18804/4205323108.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  few_shot_data = data_exp.groupby('level').apply(lambda x: x.sample(1)).reset_index(drop=True)\n"
          ]
        }
      ],
      "source": [
        "few_shot_data = data_exp.groupby('level').apply(lambda x: x.sample(1)).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV0ZBDkJCkUd"
      },
      "source": [
        "## Part 3: Using Large Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLKrG9ZqDvaS"
      },
      "source": [
        "#### API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_IeK3OuCkUd"
      },
      "outputs": [],
      "source": [
        "API_KEY = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsf1tKDBD-O1"
      },
      "source": [
        "#### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wg1UQ_VCkUd",
        "outputId": "64f7cf46-9b6f-4c8c-f307-ed35c7cef8e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"I hate myself so much that I'd rather not commit suicide just to punish myself. I'm dead on the inside.\", 'You piece of shit how Dare you make such an insensitive comment to someone that is potentially suicidal FUCK YOU!!! Have some empathy you waste of a human being.']\n",
            "[\"I'm ready. It's my own fault. I'm ready to go.\", 'When I watch films.Unable to concentrate on anything else,almost unable to take rest.']\n",
            "['Self destructive after a night out. How come im at my most fuck everything mood at the end of a night out??', \"I'm in my 50's and have been struggling with depression since I was a child. In my experience, the teen years were quite difficult, so I would say yes, that it did get better. Until it got worse again. Then better. Then worse. Then better. Many many wonderful things have happened during the good times. And I've survived 100% of my bad times. What I mostly have learned over these decades of illness, is that things always shift, eventually. I rely on docs, meds, therapy, and my spouse in bad times, and basically just ride it out.\"]\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(few_shot_data)):\n",
        "  print([few_shot_data['seeker_post'].iloc[i], sampled_data['response_post'].iloc[i]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-sdPCFZ3CkUe"
      },
      "outputs": [],
      "source": [
        "def save_response_data(prompt_name, ai_answers, rationales=None, model_name='gpt-4o'):\n",
        "  sampled_data = pd.read_csv('sampled_data.csv')\n",
        "  sampled_data['level'] = ai_answers\n",
        "  if rationales:\n",
        "    sampled_data['rationales'] = rationales\n",
        "  sampled_data.to_csv(f'coded_sampled_data_{model_name}_{prompt_name}.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "vc5JhHlACkUe"
      },
      "outputs": [],
      "source": [
        "def calculate_f1_score_debated(ai_csv_path,  ground_truth_csv_path='sampled_data.csv'):\n",
        "  ai_answers = pd.read_csv(ai_csv_path)['debated_ai_level']\n",
        "  levels = pd.read_csv(ground_truth_csv_path)['level']\n",
        "  f1 = f1_score(levels, ai_answers, average='macro')\n",
        "  print(f\"F1 Score (Debated AI): {f1:.2f}\")\n",
        "\n",
        "def calculate_f1_score_initial(csv_path, ground_truth_csv_path='sampled_data.csv'):\n",
        "  ai_answers = pd.read_csv(csv_path)['initial_ai_level']\n",
        "  levels = pd.read_csv(ground_truth_csv_path)['level']\n",
        "  f1 = f1_score(levels, ai_answers, average='macro')\n",
        "  print(f\"F1 Score (Initial AI): {f1:.2f}\")\n",
        "\n",
        "def calculate_f1_score_verifier(csv_path, ground_truth_csv_path='sampled_data.csv'):\n",
        "    ai_answers = pd.read_csv(csv_path)['verifier_suggested_level']\n",
        "    levels = pd.read_csv(ground_truth_csv_path)['level']\n",
        "    f1 = f1_score(levels, ai_answers, average='macro')\n",
        "    print(f\"F1 Score (Verifier Suggestion): {f1:.2f}\")\n",
        "\n",
        "def calculate_f1_score_roles(csv_path, ground_truth_csv_path='sampled_data.csv'):\n",
        "    ai_answers = pd.read_csv(csv_path)['roles_based_level']\n",
        "    levels = pd.read_csv(ground_truth_csv_path)['level']\n",
        "    f1 = f1_score(levels, ai_answers, average='macro')\n",
        "    print(f\"F1 Score (Verifier Suggestion): {f1:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_-HLvyGCkUe"
      },
      "outputs": [],
      "source": [
        "def calculate_f1_score_debated(ai_csv_path,  ground_truth_csv_path='sampled_data.csv'):\n",
        "  ai_answers = pd.read_csv(ai_csv_path)['debated_ai_level']\n",
        "  levels = pd.read_csv(ground_truth_csv_path)['level']\n",
        "  f1 = f1_score(levels, ai_answers, average='macro')\n",
        "  print(f\"F1 Score (Debated AI): {f1:.2f}\")\n",
        "\n",
        "def calculate_f1_score_initial(csv_path, ground_truth_csv_path='sampled_data.csv'):\n",
        "  ai_answers = pd.read_csv(csv_path)['initial_ai_level']\n",
        "  levels = pd.read_csv(ground_truth_csv_path)['level']\n",
        "  f1 = f1_score(levels, ai_answers, average='macro')\n",
        "  print(f\"F1 Score (Initial AI): {f1:.2f}\")\n",
        "\n",
        "def calculate_f1_score_verifier(csv_path, ground_truth_csv_path='sampled_data.csv'):\n",
        "    ai_answers = pd.read_csv(csv_path)['verifier_suggested_level']\n",
        "    levels = pd.read_csv(ground_truth_csv_path)['level']\n",
        "    f1 = f1_score(levels, ai_answers, average='macro')\n",
        "    print(f\"F1 Score (Verifier Suggestion): {f1:.2f}\")\n",
        "\n",
        "def calculate_f1_score_roles(csv_path, ground_truth_csv_path='sampled_data.csv'):\n",
        "    ai_answers = pd.read_csv(csv_path)['roles_based_level']\n",
        "    levels = pd.read_csv(ground_truth_csv_path)['level']\n",
        "    f1 = f1_score(levels, ai_answers, average='macro')\n",
        "    print(f\"F1 Score (Verifier Suggestion): {f1:.2f}\")\n",
        "\n",
        "def calculate_cohens_kappa_debated(csv1, ground_truth_csv_path='sampled_data.csv'):\n",
        "    df1 = pd.read_csv(csv1)\n",
        "    df2 = pd.read_csv(ground_truth_csv_path)\n",
        "    ratings1 = df1['debated_ai_level']\n",
        "    ratings2 = df2['level']\n",
        "    normal_kappa = cohen_kappa_score(ratings1, ratings2)\n",
        "    weighted_kappa = cohen_kappa_score(ratings1, ratings2, weights='quadratic')\n",
        "    print(f\"Cohen's Kappa (Debated AI): {normal_kappa:.2f}\\nWeighted Cohen's Kappa (Debated AI): {weighted_kappa:.2f}\")\n",
        "\n",
        "def calculate_cohens_kappa_initial(csv_path, ground_truth_csv_path='sampled_data.csv'):\n",
        "    df1 = pd.read_csv(csv_path)\n",
        "    df2 = pd.read_csv(ground_truth_csv_path)\n",
        "    ratings1 = df1['initial_ai_level']\n",
        "    ratings2 = df2['level']\n",
        "    normal_kappa = cohen_kappa_score(ratings1, ratings2)\n",
        "    weighted_kappa = cohen_kappa_score(ratings1, ratings2, weights='quadratic')\n",
        "    print(f\"Cohen's Kappa (Initial AI): {normal_kappa:.2f}\\nWeighted Cohen's Kappa (Initial AI): {weighted_kappa:.2f}\")\n",
        "\n",
        "def calculate_cohens_kappa_verifier(csv_path, ground_truth_csv_path='sampled_data.csv'):\n",
        "    df1 = pd.read_csv(csv_path)\n",
        "    df2 = pd.read_csv(ground_truth_csv_path)\n",
        "    ratings1 = df1['verifier_suggested_level']\n",
        "    ratings2 = df2['level']\n",
        "    normal_kappa = cohen_kappa_score(ratings1, ratings2)\n",
        "    weighted_kappa = cohen_kappa_score(ratings1, ratings2, weights='quadratic')\n",
        "    print(f\"Cohen's Kappa (Verifier Suggestion): {normal_kappa:.2f}\")\n",
        "    print(f\"Weighted Cohen's Kappa (Verifier Suggestion): {weighted_kappa:.2f}\")\n",
        "\n",
        "def calculate_cohens_kappa_roles(csv_path, ground_truth_csv_path='sampled_data.csv'):\n",
        "    df1 = pd.read_csv(csv_path)\n",
        "    df2 = pd.read_csv(ground_truth_csv_path)\n",
        "    ratings1 = df1['roles_based_level']\n",
        "    ratings2 = df2['level']\n",
        "    normal_kappa = cohen_kappa_score(ratings1, ratings2)\n",
        "    weighted_kappa = cohen_kappa_score(ratings1, ratings2, weights='quadratic')\n",
        "    print(f\"Cohen's Kappa (Verifier Suggestion): {normal_kappa:.2f}\")\n",
        "    print(f\"Weighted Cohen's Kappa (Verifier Suggestion): {weighted_kappa:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "iaC2SuqwCkUf"
      },
      "outputs": [],
      "source": [
        "def clean_json(json_str):\n",
        "  pattern = r'^```json\\s*(.*?)\\s*```$'\n",
        "  json_str = re.sub(pattern, r'\\1', json_str, flags=re.DOTALL)\n",
        "  json_str = re.sub(r'(\"Level\": )(\\w+)', r'\\1\"\\2\"', json_str)\n",
        "  json_str = json_str.replace(\"['']\", \"[]\")\n",
        "  json_str = json_str.replace(\"[\\\"\\\"]\", \"[]\")\n",
        "  return json_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "zZiASdHrCkUf"
      },
      "outputs": [],
      "source": [
        "def convert_response_data(json_ai):\n",
        "    json_answers = []\n",
        "    for i, json_str in enumerate(json_ai):\n",
        "        try:\n",
        "            json_str = clean_json(json_str)\n",
        "            data = json.loads(json_str)\n",
        "            json_answers.append(data)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error parsing item {i}: {e}\")\n",
        "            data = {\"Level\": \"C\", \"Rationale\": \"\"}\n",
        "            json_answers.append(data)\n",
        "    ai_levels = [answer_dict[answer['Level']] for answer in json_answers]\n",
        "    ai_rationales = [answer['Rationale'] for answer in json_answers]\n",
        "    return ai_levels, ai_rationales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "m5gklXaTCkUh"
      },
      "outputs": [],
      "source": [
        "def rationale_similarity(list1):\n",
        "    \"\"\"\n",
        "    Compares two lists of lists of strings and computes a similarity score based on overlapping elements.\n",
        "\n",
        "    :param list1: First list of lists of strings\n",
        "    :param list2: Second list of lists of strings\n",
        "    :return: Similarity score (0 to 1) and overlap details\n",
        "    \"\"\"\n",
        "    ground_truth_rationales = []\n",
        "    for i in range(response_count):\n",
        "        ground_truth_rationales.append([] if sampled_data['rationales'].iloc[i] is np.nan else sampled_data['rationales'].iloc[i].split(\"|\")[:-1])\n",
        "\n",
        "    list2 = ground_truth_rationales\n",
        "    if len(list1) != len(list2):\n",
        "        raise ValueError(\"Lists must have the same number of sublists\")\n",
        "\n",
        "    total_score = 0\n",
        "    total_possible = 0\n",
        "    overlap_details = []\n",
        "\n",
        "    for sub1, sub2 in zip(list1, list2):\n",
        "        set1, set2 = set(sub1), set(sub2)\n",
        "        intersection = set1 & set2\n",
        "        union = set1 | set2\n",
        "\n",
        "        similarity = len(intersection) / len(union) if union else 1\n",
        "        total_score += similarity\n",
        "        total_possible += 1\n",
        "\n",
        "        overlap_details.append({\n",
        "            \"sublist_1\": sub1,\n",
        "            \"sublist_2\": sub2,\n",
        "            \"overlap\": list(intersection),\n",
        "            \"similarity\": similarity\n",
        "        })\n",
        "\n",
        "    overall_similarity = total_score / total_possible if total_possible else 1\n",
        "    return overall_similarity, overlap_details\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXHVRQ3eCkUe"
      },
      "source": [
        "## Debate and Decide Empathy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "9485dTFLDQlz"
      },
      "outputs": [],
      "source": [
        "def debate_and_decide(seeker_post, response_post, initial_level, original_prompt, client):\n",
        "    \"\"\"\n",
        "    Uses additional agents to debate and decide on the final empathy level.\n",
        "    \"\"\"\n",
        "    # Pro-view debater\n",
        "    pro_view_prompt = f'''\n",
        "    Given the following conversation snippet:\n",
        "    Seeker Post: {seeker_post}\n",
        "    Response Post: {response_post}\n",
        "\n",
        "    And an initial classification of the response post as level \"{initial_level}\" based on this prompt \"{original_prompt}\".\n",
        "    '''\n",
        "    pro_view_completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a mental health professional arguing in favor of an empathy classification.\"},\n",
        "            {\"role\": \"user\", \"content\": pro_view_prompt}\n",
        "        ]\n",
        "    )\n",
        "    pro_view_argument = pro_view_completion.choices[0].message.content.strip()\n",
        "\n",
        "    # Anti-view debater\n",
        "    anti_view_prompt = f'''\n",
        "    Given the following conversation snippet:\n",
        "    Seeker Post: {seeker_post}\n",
        "    Response Post: {response_post}\n",
        "\n",
        "    And an initial classification of the response post as level \"{initial_level}\" based on this prompt \"{original_prompt}\".\n",
        "    Answer with the following JSON format:\n",
        "    {{\n",
        "        \"Suggested_Level\": \"A\", \"B\", or \"C\",\n",
        "        \"Argument\": \"Your reasoning here\"\n",
        "    }}\n",
        "    '''\n",
        "    anti_view_completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a mental health professional arguing against an empathy classification and suggesting an alternative.\"},\n",
        "            {\"role\": \"user\", \"content\": anti_view_prompt}\n",
        "        ]\n",
        "    )\n",
        "    anti_view_response_str = anti_view_completion.choices[0].message.content.strip()\n",
        "\n",
        "    # Attempt to parse the anti-view JSON response\n",
        "    try:\n",
        "        anti_view_data = json.loads(clean_json(anti_view_response_str))\n",
        "        anti_view_suggested_level = anti_view_data.get(\"Suggested_Level\", \"C\") # Default to C if parsing fails\n",
        "        anti_view_argument = anti_view_data.get(\"Argument\", \"Could not parse argument.\")\n",
        "    except json.JSONDecodeError:\n",
        "        anti_view_suggested_level = \"C\" # Default to C if parsing fails\n",
        "        anti_view_argument = \"Error parsing anti-view response.\"\n",
        "\n",
        "\n",
        "    # Decider\n",
        "    decider_prompt = f'''\n",
        "    Given the following conversation snippet:\n",
        "    Seeker Post: {seeker_post}\n",
        "    Response Post: {response_post}\n",
        "\n",
        "    An initial classification was made as level \"{initial_level}\" for this prompt \"{original_prompt}\".\n",
        "\n",
        "    Here is an argument against that classification, suggesting level \"{anti_view_suggested_level}\":\n",
        "    {anti_view_argument}\n",
        "\n",
        "    Here is an argument supporting that classification:\n",
        "    {pro_view_argument}\n",
        "\n",
        "\n",
        "    Considering the original texts and both arguments, decide on the final empathy level (A, B, or C) that best fits the response post based on exploration.\n",
        "    Answer with the final level (A, B, or C) and a brief rationale.\n",
        "    Answer with the following JSON format:\n",
        "    {{\n",
        "        \"Final_Level\": \"A\", \"B\", or \"C\",\n",
        "        \"Rationale\": \"Your reasoning for the final decision\"\n",
        "    }}\n",
        "    '''\n",
        "    decider_completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a neutral mental health professional evaluating arguments and making a final decision on an empathy classification.\"},\n",
        "            {\"role\": \"user\", \"content\": decider_prompt}\n",
        "        ]\n",
        "    )\n",
        "    decider_response_str = decider_completion.choices[0].message.content.strip()\n",
        "\n",
        "    # Attempt to parse the decider JSON response\n",
        "    try:\n",
        "        decider_data = json.loads(clean_json(decider_response_str))\n",
        "        final_level = decider_data.get(\"Final_Level\", \"C\") # Default to C if parsing fails\n",
        "        final_rationale = decider_data.get(\"Rationale\", \"Could not parse rationale.\")\n",
        "    except json.JSONDecodeError:\n",
        "        final_level = \"C\" # Default to C if parsing fails\n",
        "        final_rationale = \"Error parsing decider response.\"\n",
        "\n",
        "    return final_level, final_rationale, pro_view_argument, anti_view_argument, anti_view_suggested_level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "sQ9AwtnOEXUO"
      },
      "outputs": [],
      "source": [
        "\n",
        "def label_empathy(answer_file):\n",
        "    client = OpenAI(api_key=API_KEY)\n",
        "    answer_dict = {\n",
        "        \"C\": 0,\n",
        "        \"B\": 1,\n",
        "        \"A\": 2\n",
        "    }\n",
        "    \n",
        "    response_count = len(sampled_data)\n",
        "\n",
        "    # Define column names for the CSV\n",
        "    # Start with columns from sampled_data and add new AI-generated columns\n",
        "    output_columns = list(sampled_data.columns) + [\n",
        "        'initial_ai_level_char',       # e.g., \"A\", \"B\", \"C\"\n",
        "        'initial_ai_level_num',        # e.g., 2, 1, 0\n",
        "        'debated_ai_level_char',       # Final level from debate\n",
        "        'debated_ai_level_num',\n",
        "        'debated_ai_rationale',\n",
        "        'pro_view_argument',\n",
        "        'anti_view_argument',\n",
        "        'anti_view_suggested_level_char', # Suggested level by anti-view\n",
        "        'anti_view_suggested_level_num'\n",
        "    ]\n",
        "\n",
        "    # Initialize CSV file with headers (this will overwrite the file if it exists)\n",
        "    pd.DataFrame(columns=output_columns).to_csv(answer_file, index=False)\n",
        "    print(f\"Initialized {answer_file} with headers.\")\n",
        "\n",
        "    for i in tqdm(range(100)):\n",
        "        chosen_messages = [sampled_data['seeker_post'].iloc[i], sampled_data['response_post'].iloc[i]]\n",
        "        \n",
        "        # Base prompt for initial classification\n",
        "        initial_classification_prompt = f'''\n",
        "        You will be provided with a snippet of a conversation.\n",
        "        The first piece of text will be a support seeking post from an online user, and the second will be a response posted in response to the seeker's post.\n",
        "        Classify each query based on the level of empathy in the response post in the context of the seeker post.\n",
        "\n",
        "        We measure the level of empathy in terms of exploration, which is the pursuit of further information to better understand the seeker's situation.\n",
        "\n",
        "        Levels Of Empathy:\n",
        "        A. Strong Exploration: The response includes specific questions about the seeker's situation\n",
        "        B. Weak Exploration: The response includes general questions about the seeker's situation.\n",
        "        C. No Exploration: The response does not include any questions about the seeker's situation to further understand the seeker's situation.\n",
        "\n",
        "        Please classify the following responses based on the level of empathy in the response post in the context of the seeker post.\n",
        "        Seeker Post: {chosen_messages[0]}\n",
        "        Response Post: {chosen_messages[1]}\n",
        "\n",
        "        Answer with A, B, C and no justification for your answer.\n",
        "        Include no punctuation after your answer.\n",
        "        '''\n",
        "\n",
        "        current_initial_answer_char = \"C\"  # Default value\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"developer\", \"content\": \"You are a mental health professional.\"},\n",
        "                    {\"role\": \"user\", \"content\": initial_classification_prompt}\n",
        "                ]\n",
        "            )\n",
        "            raw_initial_answer = completion.choices[0].message.content.strip()\n",
        "            if raw_initial_answer in answer_dict:\n",
        "                 current_initial_answer_char = raw_initial_answer\n",
        "            else:\n",
        "                print(f\"Warning: Unexpected initial answer '{raw_initial_answer}' for item {i}. Defaulting to 'C'.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting initial AI level for item {i}: {e}. Defaulting to 'C'.\")\n",
        "            # current_initial_answer_char remains \"C\"\n",
        "\n",
        "        # Debate and Decide\n",
        "        final_level_char = \"C\" # Default\n",
        "        final_rationale = \"Error in debate\"\n",
        "        pro_arg = \"Error in debate\"\n",
        "        anti_arg = \"Error in debate\"\n",
        "        anti_level_char = \"C\" # Default\n",
        "\n",
        "        try:\n",
        "            final_level_char, final_rationale, pro_arg, anti_arg, anti_level_char = debate_and_decide(\n",
        "                chosen_messages[0],\n",
        "                chosen_messages[1],\n",
        "                current_initial_answer_char, # Pass the character A, B, or C\n",
        "                initial_classification_prompt, # Pass the original prompt text used for initial classification\n",
        "                client\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error during debate_and_decide for item {i}: {e}\")\n",
        "            # Defaults will be used\n",
        "\n",
        "        # Prepare data for the current row\n",
        "        # Start with original data from sampled_data\n",
        "        row_data_dict = sampled_data.iloc[i].to_dict()\n",
        "\n",
        "        # Add all AI generated data\n",
        "        row_data_dict['initial_ai_level_char'] = current_initial_answer_char\n",
        "        row_data_dict['initial_ai_level_num'] = answer_dict.get(current_initial_answer_char, 0)\n",
        "        \n",
        "        row_data_dict['debated_ai_level_char'] = final_level_char\n",
        "        row_data_dict['debated_ai_level_num'] = answer_dict.get(final_level_char, 0)\n",
        "        row_data_dict['debated_ai_rationale'] = final_rationale\n",
        "        \n",
        "        row_data_dict['pro_view_argument'] = pro_arg\n",
        "        row_data_dict['anti_view_argument'] = anti_arg\n",
        "        \n",
        "        row_data_dict['anti_view_suggested_level_char'] = anti_level_char\n",
        "        row_data_dict['anti_view_suggested_level_num'] = answer_dict.get(anti_level_char, 0)\n",
        "\n",
        "        # Convert the dictionary for the current row to a DataFrame\n",
        "        # Ensure the columns argument is used to maintain order and handle missing keys gracefully if any\n",
        "        current_row_df = pd.DataFrame([row_data_dict], columns=output_columns)\n",
        "        \n",
        "        # Append to CSV. mode='a' for append, header=False because headers are already written.\n",
        "        current_row_df.to_csv(answer_file, mode='a', header=False, index=False)\n",
        "\n",
        "    print(f\"Processing complete. All results saved incrementally to {answer_file}\")\n",
        "\n",
        "# ...existing code...\n",
        "# Replace the existing label_empathy function in cell eafa7076 with the one above.\n",
        "# The final block of code in your original label_empathy function that starts with:\n",
        "# output_df = sampled_data.copy()\n",
        "# ... and ends with output_df.to_csv(...)\n",
        "# is no longer needed because saving is done incrementally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_f1_score(csv_path, column_name, print_name):\n",
        "    ai_answers = pd.read_csv(csv_path)[column_name]\n",
        "    levels = pd.read_csv(csv_path)['level']\n",
        "    f1 = f1_score(levels, ai_answers, average='macro')\n",
        "    print(f\"F1 Score ({print_name}): {f1:.2f}\")\n",
        "\n",
        "def calculate_cohens_kappa(csv_path, column_name, print_name):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    ratings1 = df[column_name]\n",
        "    ratings2 = df['level']\n",
        "    normal_kappa = cohen_kappa_score(ratings1, ratings2)\n",
        "    weighted_kappa = cohen_kappa_score(ratings1, ratings2, weights='quadratic')\n",
        "    print(f\"Cohen's Kappa ({print_name}): {normal_kappa:.2f}\\nWeighted Cohen's Kappa ({print_name}): {weighted_kappa:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVm46MCtExaD",
        "outputId": "5eb20e19-e132-4bdf-e739-b237a5e2a257"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized coded_sampled_data_gpt-4o_no_shot_debated.csv with headers.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [14:07<00:00,  8.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing complete. All results saved incrementally to coded_sampled_data_gpt-4o_no_shot_debated.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "answer_file = 'coded_sampled_data_gpt-4o_no_shot_debated.csv'\n",
        "label_empathy(answer_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "vDe_s8gZE4-M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for Initial AI Response:\n",
            "F1 Score (Initial AI Level): 0.63\n",
            "Cohen's Kappa (Initial AI Level): 0.62\n",
            "Weighted Cohen's Kappa (Initial AI Level): 0.73\n",
            "\n",
            "Metrics for Debated AI Response:\n",
            "F1 Score (Debated AI Level): 0.54\n",
            "Cohen's Kappa (Debated AI Level): 0.53\n",
            "Weighted Cohen's Kappa (Debated AI Level): 0.65\n"
          ]
        }
      ],
      "source": [
        "print(\"Metrics for Initial AI Response:\")\n",
        "calculate_f1_score(answer_file,'initial_ai_level_num', 'Initial AI Level')\n",
        "calculate_cohens_kappa(answer_file, 'initial_ai_level_num', 'Initial AI Level')\n",
        "\n",
        "print(\"\\nMetrics for Debated AI Response:\")\n",
        "calculate_f1_score(answer_file, 'debated_ai_level_num', 'Debated AI Level')\n",
        "calculate_cohens_kappa(answer_file, 'debated_ai_level_num', 'Debated AI Level')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYtfdediU0cQ"
      },
      "source": [
        "## Verifyband Decide Empathy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def verifier_and_decide_empathy(seeker_post, response_post, initial_level, original_prompt, client):\n",
        "    # Verifier Agent\n",
        "    verifier_prompt = f\"\"\"\n",
        "    You are a verifier evaluating whether the initial classification of empathy level is justified.\n",
        "\n",
        "    Classification: '{initial_level}'\n",
        "\n",
        "    Seeker Post: {seeker_post}\n",
        "    Response Post: {response_post}\n",
        "\n",
        "    Based on the EPITOME rubric for Exploration, determine if this label is correct:\n",
        "    - A: Strong Exploration (asks specific questions)\n",
        "    - B: Weak Exploration (asks general questions)\n",
        "    - C: No Exploration (no questions at all)\n",
        "\n",
        "    Respond in JSON:\n",
        "    {{\n",
        "        \"Agree\": true or false,\n",
        "        \"Suggested_Level\": \"A\", \"B\", or \"C\",\n",
        "        \"Justification\": \"...\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    verifier_completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a mental health researcher verifying empathy annotations.\"},\n",
        "            {\"role\": \"user\", \"content\": verifier_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    verifier_raw = verifier_completion.choices[0].message.content.strip()\n",
        "    try:\n",
        "        verifier_data = json.loads(clean_json(verifier_raw))\n",
        "        agree = verifier_data.get(\"Agree\", False)\n",
        "        suggested_level = verifier_data.get(\"Suggested_Level\", \"C\")\n",
        "        justification = verifier_data.get(\"Justification\", \"\")\n",
        "    except:\n",
        "        agree = False\n",
        "        suggested_level = \"C\"\n",
        "        justification = \"Could not parse verifier response.\"\n",
        "\n",
        "    # Decider Agent\n",
        "    decider_prompt = f\"\"\"\n",
        "    You are a neutral adjudicator evaluating the empathy level of a response post using the EPITOME rubric.\n",
        "\n",
        "    Initial Classification: '{initial_level}'\n",
        "\n",
        "    Seeker: {seeker_post}\n",
        "    Response: {response_post}\n",
        "\n",
        "    Verifier says:\n",
        "    - Agrees with initial: {agree}\n",
        "    - Suggested_Level: {suggested_level}\n",
        "    - Justification: {justification}\n",
        "\n",
        "    Make a final decision: A, B, or C.\n",
        "\n",
        "    Respond in JSON:\n",
        "    {{\n",
        "        \"Final_Level\": \"A\", \"B\", or \"C\",\n",
        "        \"Rationale\": \"Why this final label best fits the rubric and context\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    decider_completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a neutral mental health expert finalizing the empathy classification.\"},\n",
        "            {\"role\": \"user\", \"content\": decider_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        decider_data = json.loads(clean_json(decider_completion.choices[0].message.content.strip()))\n",
        "        final_level = decider_data.get(\"Final_Level\", \"C\")\n",
        "        final_rationale = decider_data.get(\"Rationale\", \"\")\n",
        "    except:\n",
        "        final_level = \"C\"\n",
        "        final_rationale = \"Could not parse rationale.\"\n",
        "\n",
        "    return final_level, final_rationale, justification, agree, suggested_level\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prr945vQWcXW"
      },
      "outputs": [],
      "source": [
        "# def label_empathy(answer_file):\n",
        "#     client = OpenAI(api_key=API_KEY)\n",
        "#     answer_dict = {\n",
        "#     \"C\": 0,\n",
        "#     \"B\": 1,\n",
        "#     \"A\": 2\n",
        "#     }\n",
        "#     ai_answers_with_debate = []\n",
        "#     final_rationales = []\n",
        "#     pro_view_args = []\n",
        "#     anti_view_args = []\n",
        "#     anti_view_suggested_levels = []\n",
        "\n",
        "#     response_count = len(sampled_data)\n",
        "\n",
        "#     for i in tqdm(range(response_count)):\n",
        "#         chosen_messages = [sampled_data['seeker_post'].iloc[i], sampled_data['response_post'].iloc[i]]\n",
        "#         prompt = f'''\n",
        "#         You will be provided with a snippet of a conversation.\n",
        "#         The first piece of text will be a support seeking post from an online user, and the second will be a response posted in response to the seeker's post.\n",
        "#         Classify each query based on the level of empathy in the response post in the context of the seeker post.\n",
        "\n",
        "#         We measure the level of empathy in terms of exploration, which is the pursuit of further information to better understand the seeker's situation.\n",
        "\n",
        "#         Levels Of Empathy:\n",
        "#         A. Strong Exploration: The response includes specific questions about the seeker's situation\n",
        "\n",
        "#         B. Weak Exploration: The response includes general questions about the seeker's situation.\n",
        "\n",
        "#         C. No Exploration: The response does not include any questions about the seeker's situation to further understand the seeker's situation.\n",
        "\n",
        "\n",
        "#         Please classify the following responses based on the level of empathy in the response post in the context of the seeker post.\n",
        "#         Seeker Post: {chosen_messages[0]}\n",
        "#         Response Post: {chosen_messages[1]}\n",
        "\n",
        "#         Answer with A, B, C and no justification for your answer.\n",
        "#         Include no punctuation after your answer.\n",
        "#         '''\n",
        "\n",
        "#         completion = client.chat.completions.create(\n",
        "#             model=\"gpt-4o\",\n",
        "#             messages=[\n",
        "#                 {\"role\": \"developer\", \"content\": \"You are a mental health professional.\"},\n",
        "#                 {\"role\": \"user\", \"content\": prompt}\n",
        "#             ]\n",
        "#         )\n",
        "#         initial_answer = completion.choices[0].message.content.strip()\n",
        "\n",
        "#         # Debate and Decide\n",
        "#         final_level, final_rationale, justification, agree, suggested_level = verifier_and_decide_empathy(\n",
        "#             chosen_messages[0],\n",
        "#             chosen_messages[1],\n",
        "#             initial_answer,\n",
        "#             prompt,\n",
        "#             client\n",
        "#         )\n",
        "\n",
        "#         ai_answers_with_debate.append(answer_dict.get(final_level, 0))\n",
        "#         final_rationales.append(final_rationale)\n",
        "#         pro_view_args.append(justification)\n",
        "#         anti_view_args.append(agree)\n",
        "#         anti_view_suggested_levels.append(suggested_level)\n",
        "\n",
        "\n",
        "#     # Save the results (you might want to save more than just the final level and rationale)\n",
        "#     output_df = sampled_data.copy()\n",
        "#     output_df['initial_ai_level'] = [answer_dict.get(lvl, 0) for lvl in [completion.choices[0].message.content.strip() for completion in [client.chat.completions.create(model=\"gpt-4o\", messages=[{\"role\": \"developer\", \"content\": \"You are a mental health professional.\"}, {\"role\": \"user\", \"content\": f'''You will be provided with a snippet of a conversation.\\nThe first piece of text will be a support seeking post from an online user, and the second will be a response posted in response to the seeker's post.\\nClassify each query based on the level of empathy in the response post in the context of the seeker post.\\n\\nWe measure the level of empathy in terms of exploration, which is the pursuit of further information to better understand the seeker's situation.\\n\\nLevels Of Empathy:\\nA. Strong Exploration: The response includes specific questions about the seeker's situation\\n\\nB. Weak Exploration: The response includes general questions about the seeker's situation.\\n\\nC. No Exploration: The response does not include any questions about the seeker's situation to further understand the seeker's situation.\\n\\nPlease classify the following responses based on the level of empathy in the response post in the context of the seeker post.\\nSeeker Post: {sampled_data['seeker_post'].iloc[i]}\\nResponse Post: {sampled_data['response_post'].iloc[i]}\\n\\nAnswer with A, B, C and no justification for your answer.\\nInclude no punctuation after your answer.\\n'''}]) for i in range(response_count)]]] # This will be the initial level\n",
        "#     output_df['debated_ai_level'] = ai_answers_with_debate\n",
        "#     output_df['debated_ai_rationale'] = final_rationales\n",
        "#     output_df['verifier_justification'] = pro_view_args\n",
        "#     output_df['verifier_agreed'] = anti_view_args\n",
        "#     output_df['verifier_suggested_level'] = [answer_dict.get(lvl, 0) for lvl in anti_view_suggested_levels]\n",
        "\n",
        "#     output_df.to_csv(answer_file, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def label_empathy(answer_file):\n",
        "    client = OpenAI(api_key=API_KEY)\n",
        "    answer_dict = {\n",
        "        \"C\": 0,\n",
        "        \"B\": 1,\n",
        "        \"A\": 2\n",
        "    }\n",
        "    \n",
        "    # Determine the number of samples to process, e.g., 100 for testing or len(sampled_data) for all\n",
        "    # For now, let's stick to the first 100 as in the previous example, or use len(sampled_data) for all\n",
        "    # response_count = 100 \n",
        "    response_count = len(sampled_data) # Or use a smaller number for testing, e.g., 100\n",
        "\n",
        "    # Define column names for the CSV\n",
        "    # Start with columns from sampled_data and add new AI-generated columns\n",
        "    output_columns = list(sampled_data.columns) + [\n",
        "        'initial_ai_level_char',      # e.g., \"A\", \"B\", \"C\"\n",
        "        'initial_ai_level_num',       # e.g., 0, 1, 2\n",
        "        'final_level_char',           # Final level from verifier_and_decide\n",
        "        'final_level_num',\n",
        "        'final_rationale',\n",
        "        'verifier_justification',\n",
        "        'verifier_agreed',            # true/false\n",
        "        'verifier_suggested_level_char',\n",
        "        'verifier_suggested_level_num'\n",
        "    ]\n",
        "\n",
        "    # Initialize CSV file with headers (this will overwrite the file if it exists)\n",
        "    pd.DataFrame(columns=output_columns).to_csv(answer_file, index=False)\n",
        "    print(f\"Initialized {answer_file} with headers.\")\n",
        "\n",
        "    for i in tqdm(range(response_count)): # Loop through the specified number of samples\n",
        "        chosen_messages = [sampled_data['seeker_post'].iloc[i], sampled_data['response_post'].iloc[i]]\n",
        "        \n",
        "        # Base prompt for initial classification\n",
        "        prompt = f'''\n",
        "        You will be provided with a snippet of a conversation.\n",
        "        The first piece of text will be a support seeking post from an online user, and the second will be a response posted in response to the seeker's post.\n",
        "        Classify each query based on the level of empathy in the response post in the context of the seeker post.\n",
        "\n",
        "        We measure the level of empathy in terms of exploration, which is the pursuit of further information to better understand the seeker's situation.\n",
        "\n",
        "        Levels Of Empathy:\n",
        "        A. Strong Exploration: The response includes specific questions about the seeker's situation\n",
        "        B. Weak Exploration: The response includes general questions about the seeker's situation.\n",
        "        C. No Exploration: The response does not include any questions about the seeker's situation to further understand the seeker's situation.\n",
        "\n",
        "        Please classify the following responses based on the level of empathy in the response post in the context of the seeker post.\n",
        "        Seeker Post: {chosen_messages[0]}\n",
        "        Response Post: {chosen_messages[1]}\n",
        "\n",
        "        Answer with A, B, C and no justification for your answer.\n",
        "        Include no punctuation after your answer.\n",
        "        '''\n",
        "\n",
        "        current_initial_answer_char = \"C\"  # Default value\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"developer\", \"content\": \"You are a mental health professional.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "            raw_initial_answer = completion.choices[0].message.content.strip()\n",
        "            if raw_initial_answer in answer_dict: # Check if the raw answer is a valid key\n",
        "                 current_initial_answer_char = raw_initial_answer\n",
        "            else:\n",
        "                print(f\"Warning: Unexpected initial answer '{raw_initial_answer}' for item {i}. Defaulting to 'C'.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting initial AI level for item {i}: {e}. Defaulting to 'C'.\")\n",
        "            # current_initial_answer_char remains \"C\"\n",
        "\n",
        "        # Verifier and Decide\n",
        "        final_level_char = \"C\" # Default\n",
        "        final_rationale_text = \"Error in verifier/decide\"\n",
        "        verifier_justification_text = \"Error in verifier/decide\"\n",
        "        verifier_agreed_bool = False # Default\n",
        "        verifier_suggested_char = \"C\" # Default\n",
        "\n",
        "        try:\n",
        "            # Ensure verifier_and_decide_empathy is defined and returns these 5 values\n",
        "            final_level_char, final_rationale_text, verifier_justification_text, verifier_agreed_bool, verifier_suggested_char = verifier_and_decide_empathy(\n",
        "                chosen_messages[0],\n",
        "                chosen_messages[1],\n",
        "                current_initial_answer_char, # Pass the character A, B, or C\n",
        "                prompt, # Pass the original prompt text\n",
        "                client\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error during verifier_and_decide_empathy for item {i}: {e}\")\n",
        "            # Defaults will be used\n",
        "\n",
        "        # Prepare data for the current row\n",
        "        row_data_dict = sampled_data.iloc[i].to_dict()\n",
        "\n",
        "        # Add all AI generated data\n",
        "        row_data_dict['initial_ai_level_char'] = current_initial_answer_char\n",
        "        row_data_dict['initial_ai_level_num'] = answer_dict.get(current_initial_answer_char, 0)\n",
        "        \n",
        "        row_data_dict['final_level_char'] = final_level_char\n",
        "        row_data_dict['final_level_num'] = answer_dict.get(final_level_char, 0)\n",
        "        row_data_dict['final_rationale'] = final_rationale_text\n",
        "        \n",
        "        row_data_dict['verifier_justification'] = verifier_justification_text\n",
        "        row_data_dict['verifier_agreed'] = verifier_agreed_bool\n",
        "        \n",
        "        row_data_dict['verifier_suggested_level_char'] = verifier_suggested_char\n",
        "        row_data_dict['verifier_suggested_level_num'] = answer_dict.get(verifier_suggested_char, 0)\n",
        "\n",
        "        # Convert the dictionary for the current row to a DataFrame\n",
        "        current_row_df = pd.DataFrame([row_data_dict], columns=output_columns)\n",
        "        \n",
        "        # Append to CSV. mode='a' for append, header=False because headers are already written.\n",
        "        current_row_df.to_csv(answer_file, mode='a', header=False, index=False)\n",
        "\n",
        "    print(f\"Processing complete. All results saved incrementally to {answer_file}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "LOd3oLT5W3e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized coded_sampled_data_gpt-4o_no_shot_verifier.csv with headers.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [1:38:00<00:00,  5.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing complete. All results saved incrementally to coded_sampled_data_gpt-4o_no_shot_verifier.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "answer_file = 'coded_sampled_data_gpt-4o_no_shot_verifier.csv'\n",
        "label_empathy(answer_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "4VIgReakW7h7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for Initial AI Response:\n",
            "F1 Score (Initial AI Level): 0.64\n",
            "Cohen's Kappa (Initial AI Level): 0.62\n",
            "Weighted Cohen's Kappa (Initial AI Level): 0.74\n",
            "\n",
            "Metrics for Final AI Response:\n",
            "F1 Score (Final AI Level): 0.58\n",
            "Cohen's Kappa (Final AI Level): 0.52\n",
            "Weighted Cohen's Kappa (Final AI Level): 0.65\n"
          ]
        }
      ],
      "source": [
        "print(\"Metrics for Initial AI Response:\")\n",
        "calculate_f1_score(answer_file,'initial_ai_level_num', 'Initial AI Level')\n",
        "calculate_cohens_kappa(answer_file, 'initial_ai_level_num', 'Initial AI Level')\n",
        "\n",
        "print(\"\\nMetrics for Final AI Response:\")\n",
        "calculate_f1_score(answer_file, 'final_level_num', 'Final AI Level')\n",
        "calculate_cohens_kappa(answer_file, 'final_level_num', 'Final AI Level')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boAC0WAUMtEz"
      },
      "source": [
        "## Roles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piulxHx0MmtA"
      },
      "outputs": [],
      "source": [
        "# def label_empathy(answer_file):\n",
        "#     client = OpenAI(api_key=API_KEY)\n",
        "#     answer_dict = {\n",
        "#     \"C\": 0,\n",
        "#     \"B\": 1,\n",
        "#     \"A\": 2\n",
        "#     }\n",
        "#     initial_ai_levels = [] # To store the results from the initial prompt\n",
        "#     ai_answers_with_roles = [] # To store the final roles-based majority vote\n",
        "#     role_responses_list = [] # To store the individual responses from each role\n",
        "\n",
        "#     response_count = len(sampled_data)\n",
        "\n",
        "#     for i in tqdm(range(response_count)):\n",
        "#         seeker_post = sampled_data['seeker_post'].iloc[i]\n",
        "#         response_post = sampled_data['response_post'].iloc[i]\n",
        "#         chosen_messages = [seeker_post, response_post] # Keep this for consistency with the original prompt format\n",
        "\n",
        "#         # 1. Get the initial classification using the original prompt\n",
        "#         initial_prompt_text = f'''\n",
        "#         You will be provided with a snippet of a conversation.\n",
        "#         The first piece of text will be a support seeking post from an online user, and the second will be a response posted in response to the seeker's post.\n",
        "#         Classify each query based on the level of empathy in the response post in the context of the seeker post.\n",
        "\n",
        "#         We measure the level of empathy in terms of exploration, which is the pursuit of further information to better understand the seeker's situation.\n",
        "\n",
        "#         Levels Of Empathy:\n",
        "#         A. Strong Exploration: The response includes specific questions about the seeker's situation\n",
        "\n",
        "#         B. Weak Exploration: The response includes general questions about the seeker's situation.\n",
        "\n",
        "#         C. No Exploration: The response does not include any questions about the seeker's situation to further understand the seeker's situation.\n",
        "\n",
        "\n",
        "#         Please classify the following responses based on the level of empathy in the response post in the context of the seeker post.\n",
        "#         Seeker Post: {chosen_messages[0]}\n",
        "#         Response Post: {chosen_messages[1]}\n",
        "\n",
        "#         Answer with A, B, C and no justification for your answer.\n",
        "#         Include no punctuation after your answer.\n",
        "#         '''\n",
        "#         try:\n",
        "#             initial_completion = client.chat.completions.create(\n",
        "#                 model=\"gpt-4o\",\n",
        "#                 messages=[\n",
        "#                     {\"role\": \"user\", \"content\": initial_prompt_text}\n",
        "#                 ]\n",
        "#             )\n",
        "#             initial_answer = initial_completion.choices[0].message.content.strip()\n",
        "#             # Extract the single letter classification\n",
        "#             match = re.search(r'[ABC]', initial_answer)\n",
        "#             if match:\n",
        "#                 initial_level_letter = match.group(0)\n",
        "#             else:\n",
        "#                 initial_level_letter = \"C\" # Default if classification is not found\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error with API call for initial prompt item {i}: {e}\")\n",
        "#             initial_level_letter = \"C\" # Default on API error\n",
        "\n",
        "#         initial_ai_levels.append(answer_dict.get(initial_level_letter, 0))\n",
        "\n",
        "\n",
        "#         # 2. Define the empathy role-playing prompts, including the original prompt text\n",
        "#         empathy_role_prompts = [\n",
        "#             f\"\"\"You are a clinical mental health worker evaluating online support messages using the EPITOME framework.\n",
        "\n",
        "# Here is the original task description you are working within:\n",
        "# '''\n",
        "# {initial_prompt_text}\n",
        "# '''\n",
        "\n",
        "# Context: A user has shared a personal post seeking emotional support. A peer has responded to it. Your role is to assess whether the peer made a genuine effort to understand the user's situation by asking meaningful questions.\n",
        "\n",
        "# EPITOME Exploration Labels:\n",
        "# A — Strong Exploration: Specific, detailed questions showing real curiosity about the user's circumstances.\n",
        "# B — Weak Exploration: General or vague questions that show minimal probing.\n",
        "# C — No Exploration: No follow-up questions or attempts to better understand the user's situation.\n",
        "\n",
        "# Seeker's Post: {seeker_post}\n",
        "# Response: {response_post}\n",
        "\n",
        "# Classify the response as A, B, or C. Do not include explanation or punctuation.\"\"\",\n",
        "\n",
        "#             f\"\"\"You are a trained peer support volunteer on an emotional support platform.\n",
        "\n",
        "# Here is the original task description you are working within:\n",
        "# '''\n",
        "# {initial_prompt_text}\n",
        "# '''\n",
        "\n",
        "# Your goal is to reflect on whether the responder is actively trying to support the seeker by asking follow-up questions that show engagement and a desire to understand.\n",
        "\n",
        "# Classification:\n",
        "# - A: Strong Exploration — Very specific questions like “When did this start?” or “What happened after that?”\n",
        "# - B: Weak Exploration — Vague or surface-level questions like “Are you okay?” or “What’s going on?”\n",
        "# - C: No Exploration — The response is about the responder’s own experience or does not ask anything.\n",
        "\n",
        "# Seeker: {seeker_post}\n",
        "# Response: {response_post}\n",
        "\n",
        "# Give only one letter: A, B, or C (no punctuation or commentary).\"\"\",\n",
        "\n",
        "#             f\"\"\"You are an AI assistant trained to analyze empathetic language based on clinical labeling of peer responses.\n",
        "\n",
        "# Here is the original task description you are working within:\n",
        "# '''\n",
        "# {initial_prompt_text}\n",
        "# '''\n",
        "\n",
        "# Task: Evaluate if the responder engaged in ‘exploration’—that is, asking questions to understand the seeker better.\n",
        "\n",
        "# Labels:\n",
        "# A — Strong Exploration: Specific, contextual questions showing thoughtful support.\n",
        "# B — Weak Exploration: General or vague questions showing minimal probing.\n",
        "# C — No Exploration: The response offers no questions to understand the seeker better.\n",
        "\n",
        "# Seeker: {seeker_post}\n",
        "# Response: {response_post}\n",
        "\n",
        "# Provide only the letter (A, B, or C). No extra text.\n",
        "# \"\"\"\n",
        "#         ]\n",
        "\n",
        "#         # 3. Collect responses from each role\n",
        "#         role_responses = []\n",
        "#         for role_prompt in empathy_role_prompts:\n",
        "#             try:\n",
        "#                 completion = client.chat.completions.create(\n",
        "#                     model=\"gpt-4o\",\n",
        "#                     messages=[\n",
        "#                         {\"role\": \"user\", \"content\": role_prompt}\n",
        "#                     ]\n",
        "#                 )\n",
        "#                 response = completion.choices[0].message.content.strip()\n",
        "#                 # Extract the single letter classification\n",
        "#                 match = re.search(r'[ABC]', response)\n",
        "#                 if match:\n",
        "#                     role_responses.append(match.group(0))\n",
        "#                 else:\n",
        "#                     role_responses.append(\"C\") # Default to C if classification is not found\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error with API call for item {i} and a role: {e}\")\n",
        "#                 role_responses.append(\"C\") # Default to C on API error\n",
        "\n",
        "#         role_responses_list.append(\", \".join(role_responses))\n",
        "\n",
        "\n",
        "#         # 4. Simple majority vote for the final roles-based decision\n",
        "#         from collections import Counter\n",
        "#         vote_counts = Counter(role_responses)\n",
        "#         # Get the most common label. If there's a tie, choose one (e.g., the first one).\n",
        "#         final_level_letter = vote_counts.most_common(1)[0][0]\n",
        "#         final_level_numeric = answer_dict.get(final_level_letter, 0)\n",
        "\n",
        "#         ai_answers_with_roles.append(final_level_numeric)\n",
        "\n",
        "\n",
        "#     # 5. Save the results\n",
        "#     output_df = sampled_data.copy()\n",
        "#     output_df['initial_ai_level'] = initial_ai_levels # Store the initial classification\n",
        "#     output_df['roles_based_level'] = ai_answers_with_roles # Store the majority vote result\n",
        "#     output_df['role_individual_responses'] = role_responses_list # Store individual role responses\n",
        "\n",
        "\n",
        "#     output_df.to_csv(answer_file, index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def label_empathy(answer_file):\n",
        "    client = OpenAI(api_key=API_KEY)\n",
        "    answer_dict = {\n",
        "        \"C\": 0,\n",
        "        \"B\": 1,\n",
        "        \"A\": 2\n",
        "    }\n",
        "    \n",
        "    # Determine the number of samples to process\n",
        "    response_count = len(sampled_data) # Or use a smaller number for testing, e.g., 100\n",
        "\n",
        "    # Define column names for the CSV\n",
        "    output_columns = list(sampled_data.columns) + [\n",
        "        'initial_ai_level_char',       # e.g., \"A\", \"B\", \"C\"\n",
        "        'initial_ai_level_num',        # e.g., 0, 1, 2\n",
        "        'roles_based_level_char',      # Final level from roles\n",
        "        'roles_based_level_num',\n",
        "        'role_individual_responses_str' # Comma-separated string of role responses\n",
        "    ]\n",
        "\n",
        "    # Initialize CSV file with headers (this will overwrite the file if it exists)\n",
        "    pd.DataFrame(columns=output_columns).to_csv(answer_file, index=False)\n",
        "    print(f\"Initialized {answer_file} with headers.\")\n",
        "\n",
        "    for i in tqdm(range(response_count)): # Loop through the specified number of samples\n",
        "        seeker_post = sampled_data['seeker_post'].iloc[i]\n",
        "        response_post = sampled_data['response_post'].iloc[i]\n",
        "        chosen_messages = [seeker_post, response_post]\n",
        "\n",
        "        # 1. Get the initial classification using the original prompt\n",
        "        initial_prompt_text = f'''\n",
        "        You will be provided with a snippet of a conversation.\n",
        "        The first piece of text will be a support seeking post from an online user, and the second will be a response posted in response to the seeker's post.\n",
        "        Classify each query based on the level of empathy in the response post in the context of the seeker post.\n",
        "\n",
        "        We measure the level of empathy in terms of exploration, which is the pursuit of further information to better understand the seeker's situation.\n",
        "\n",
        "        Levels Of Empathy:\n",
        "        A. Strong Exploration: The response includes specific questions about the seeker's situation\n",
        "        B. Weak Exploration: The response includes general questions about the seeker's situation.\n",
        "        C. No Exploration: The response does not include any questions about the seeker's situation to further understand the seeker's situation.\n",
        "\n",
        "        Please classify the following responses based on the level of empathy in the response post in the context of the seeker post.\n",
        "        Seeker Post: {chosen_messages[0]}\n",
        "        Response Post: {chosen_messages[1]}\n",
        "\n",
        "        Answer with A, B, C and no justification for your answer.\n",
        "        Include no punctuation after your answer.\n",
        "        '''\n",
        "        current_initial_level_char = \"C\" # Default\n",
        "        try:\n",
        "            initial_completion = client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": initial_prompt_text}\n",
        "                ]\n",
        "            )\n",
        "            initial_answer = initial_completion.choices[0].message.content.strip()\n",
        "            match = re.search(r'[ABC]', initial_answer)\n",
        "            if match:\n",
        "                current_initial_level_char = match.group(0)\n",
        "            else:\n",
        "                print(f\"Warning: Could not extract A, B, or C from initial answer '{initial_answer}' for item {i}. Defaulting to 'C'.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error with API call for initial prompt item {i}: {e}. Defaulting to 'C'.\")\n",
        "        \n",
        "        current_initial_level_num = answer_dict.get(current_initial_level_char, 0)\n",
        "\n",
        "        # 2. Define the empathy role-playing prompts\n",
        "        empathy_role_prompts = [\n",
        "            f\"\"\"You are a clinical mental health worker evaluating online support messages using the EPITOME framework.\n",
        "Here is the original task description you are working within:\n",
        "'''\n",
        "{initial_prompt_text}\n",
        "'''\n",
        "Context: A user has shared a personal post seeking emotional support. A peer has responded to it. Your role is to assess whether the peer made a genuine effort to understand the user's situation by asking meaningful questions.\n",
        "EPITOME Exploration Labels:\n",
        "A — Strong Exploration: Specific, detailed questions showing real curiosity about the user's circumstances.\n",
        "B — Weak Exploration: General or vague questions that show minimal probing.\n",
        "C — No Exploration: No follow-up questions or attempts to better understand the user's situation.\n",
        "Seeker's Post: {seeker_post}\n",
        "Response: {response_post}\n",
        "Classify the response as A, B, or C. Do not include explanation or punctuation.\"\"\",\n",
        "\n",
        "            f\"\"\"You are a trained peer support volunteer on an emotional support platform.\n",
        "Here is the original task description you are working within:\n",
        "'''\n",
        "{initial_prompt_text}\n",
        "'''\n",
        "Your goal is to reflect on whether the responder is actively trying to support the seeker by asking follow-up questions that show engagement and a desire to understand.\n",
        "Classification:\n",
        "- A: Strong Exploration — Very specific questions like “When did this start?” or “What happened after that?”\n",
        "- B: Weak Exploration — Vague or surface-level questions like “Are you okay?” or “What’s going on?”\n",
        "- C: No Exploration — The response is about the responder’s own experience or does not ask anything.\n",
        "Seeker: {seeker_post}\n",
        "Response: {response_post}\n",
        "Give only one letter: A, B, or C (no punctuation or commentary).\"\"\",\n",
        "\n",
        "            f\"\"\"You are an AI assistant trained to analyze empathetic language based on clinical labeling of peer responses.\n",
        "Here is the original task description you are working within:\n",
        "'''\n",
        "{initial_prompt_text}\n",
        "'''\n",
        "Task: Evaluate if the responder engaged in ‘exploration’—that is, asking questions to understand the seeker better.\n",
        "Labels:\n",
        "A — Strong Exploration: Specific, contextual questions showing thoughtful support.\n",
        "B — Weak Exploration: General or vague questions showing minimal probing.\n",
        "C — No Exploration: The response offers no questions to understand the seeker better.\n",
        "Seeker: {seeker_post}\n",
        "Response: {response_post}\n",
        "Provide only the letter (A, B, or C). No extra text.\"\"\"\n",
        "        ]\n",
        "\n",
        "        # 3. Collect responses from each role\n",
        "        current_role_responses_char = []\n",
        "        for role_idx, role_prompt in enumerate(empathy_role_prompts):\n",
        "            role_response_char = \"C\" # Default for this role\n",
        "            try:\n",
        "                completion = client.chat.completions.create(\n",
        "                    model=\"gpt-4o\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"user\", \"content\": role_prompt}\n",
        "                    ]\n",
        "                )\n",
        "                response = completion.choices[0].message.content.strip()\n",
        "                match = re.search(r'[ABC]', response)\n",
        "                if match:\n",
        "                    role_response_char = match.group(0)\n",
        "                else:\n",
        "                    print(f\"Warning: Could not extract A, B, or C from role {role_idx} answer '{response}' for item {i}. Defaulting to 'C'.\")\n",
        "                current_role_responses_char.append(role_response_char)\n",
        "            except Exception as e:\n",
        "                print(f\"Error with API call for item {i}, role {role_idx}: {e}. Defaulting to 'C'.\")\n",
        "                current_role_responses_char.append(\"C\") # Default to C on API error\n",
        "        \n",
        "        current_role_responses_str = \", \".join(current_role_responses_char)\n",
        "\n",
        "        # 4. Simple majority vote for the final roles-based decision\n",
        "        from collections import Counter\n",
        "        vote_counts = Counter(current_role_responses_char)\n",
        "        \n",
        "        final_roles_level_char = \"C\" # Default if no responses or tie with no clear winner\n",
        "        if vote_counts:\n",
        "            # Get the most common label. If there's a tie, Counter.most_common(1) picks one.\n",
        "            # You might want more sophisticated tie-breaking if needed.\n",
        "            final_roles_level_char = vote_counts.most_common(1)[0][0]\n",
        "        \n",
        "        final_roles_level_num = answer_dict.get(final_roles_level_char, 0)\n",
        "\n",
        "        # Prepare data for the current row\n",
        "        row_data_dict = sampled_data.iloc[i].to_dict()\n",
        "\n",
        "        # Add all AI generated data\n",
        "        row_data_dict['initial_ai_level_char'] = current_initial_level_char\n",
        "        row_data_dict['initial_ai_level_num'] = current_initial_level_num\n",
        "        \n",
        "        row_data_dict['roles_based_level_char'] = final_roles_level_char\n",
        "        row_data_dict['roles_based_level_num'] = final_roles_level_num\n",
        "        row_data_dict['role_individual_responses_str'] = current_role_responses_str\n",
        "        \n",
        "        # Convert the dictionary for the current row to a DataFrame\n",
        "        current_row_df = pd.DataFrame([row_data_dict], columns=output_columns)\n",
        "        \n",
        "        # Append to CSV. mode='a' for append, header=False because headers are already written.\n",
        "        current_row_df.to_csv(answer_file, mode='a', header=False, index=False)\n",
        "\n",
        "    print(f\"Processing complete. All results saved incrementally to {answer_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "vQHoaBolP3zI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized coded_sampled_data_gpt-4o_no_shot_roles.csv with headers.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 22/1000 [00:49<36:44,  2.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error with API call for item 22, role 1: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-eMLxU7I2FH1eVZlBr5fna3f7 on tokens per min (TPM): Limit 30000, Used 29006, Requested 1237. Please try again in 486ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Defaulting to 'C'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 23/1000 [00:56<1:01:46,  3.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error with API call for item 23, role 1: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-eMLxU7I2FH1eVZlBr5fna3f7 on tokens per min (TPM): Limit 30000, Used 29549, Requested 596. Please try again in 290ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Defaulting to 'C'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▎         | 25/1000 [01:04<1:03:35,  3.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error with API call for item 25, role 0: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-eMLxU7I2FH1eVZlBr5fna3f7 on tokens per min (TPM): Limit 30000, Used 29473, Requested 585. Please try again in 116ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Defaulting to 'C'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 26/1000 [01:07<1:00:19,  3.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error with API call for item 26, role 1: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-eMLxU7I2FH1eVZlBr5fna3f7 on tokens per min (TPM): Limit 30000, Used 29181, Requested 1127. Please try again in 616ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Defaulting to 'C'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 30/1000 [01:32<1:25:04,  5.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error with API call for item 29, role 2: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-eMLxU7I2FH1eVZlBr5fna3f7 on tokens per min (TPM): Limit 30000, Used 29558, Requested 569. Please try again in 254ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Defaulting to 'C'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 39/1000 [02:25<1:28:19,  5.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error with API call for item 39, role 0: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-eMLxU7I2FH1eVZlBr5fna3f7 on tokens per min (TPM): Limit 30000, Used 29691, Requested 554. Please try again in 489ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Defaulting to 'C'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 41/1000 [02:32<1:10:24,  4.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error with API call for item 41, role 0: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-eMLxU7I2FH1eVZlBr5fna3f7 on tokens per min (TPM): Limit 30000, Used 29585, Requested 771. Please try again in 712ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Defaulting to 'C'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 43/1000 [02:42<1:13:45,  4.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error with API call for item 43, role 1: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-eMLxU7I2FH1eVZlBr5fna3f7 on tokens per min (TPM): Limit 30000, Used 29616, Requested 542. Please try again in 316ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Defaulting to 'C'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▍         | 48/1000 [03:05<1:17:13,  4.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error with API call for item 47, role 2: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-eMLxU7I2FH1eVZlBr5fna3f7 on tokens per min (TPM): Limit 30000, Used 29370, Requested 636. Please try again in 12ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Defaulting to 'C'.\n",
            "Error with API call for item 48, role 0: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-eMLxU7I2FH1eVZlBr5fna3f7 on tokens per min (TPM): Limit 30000, Used 29228, Requested 777. Please try again in 10ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Defaulting to 'C'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 489/1000 [32:30<34:33,  4.06s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error with API call for item 488, role 2: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-eMLxU7I2FH1eVZlBr5fna3f7 on tokens per min (TPM): Limit 30000, Used 29556, Requested 642. Please try again in 396ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Defaulting to 'C'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|█████▉    | 599/1000 [39:56<21:22,  3.20s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error with API call for initial prompt item 599: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-eMLxU7I2FH1eVZlBr5fna3f7 on tokens per min (TPM): Limit 30000, Used 29756, Requested 423. Please try again in 358ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Defaulting to 'C'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 729/1000 [48:42<41:01,  9.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error with API call for initial prompt item 729: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-eMLxU7I2FH1eVZlBr5fna3f7 on tokens per min (TPM): Limit 30000, Used 29888, Requested 333. Please try again in 442ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}. Defaulting to 'C'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [1:06:09<00:00,  3.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing complete. All results saved incrementally to coded_sampled_data_gpt-4o_no_shot_roles.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "answer_file_roles = 'coded_sampled_data_gpt-4o_no_shot_roles.csv'\n",
        "label_empathy(answer_file_roles)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "CYBRZ9bsSops"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for Initial AI Response:\n",
            "F1 Score (Initial AI Level): 0.64\n",
            "Cohen's Kappa (Initial AI Level): 0.64\n",
            "Weighted Cohen's Kappa (Initial AI Level): 0.76\n",
            "\n",
            "Metrics for Final AI Response:\n",
            "F1 Score (Final AI Level): 0.70\n",
            "Cohen's Kappa (Final AI Level): 0.70\n",
            "Weighted Cohen's Kappa (Final AI Level): 0.80\n"
          ]
        }
      ],
      "source": [
        "print(\"Metrics for Initial AI Response:\")\n",
        "calculate_f1_score(answer_file_roles,'initial_ai_level_num', 'Initial AI Level')\n",
        "calculate_cohens_kappa(answer_file_roles, 'initial_ai_level_num', 'Initial AI Level')\n",
        "\n",
        "print(\"\\nMetrics for Final AI Response:\")\n",
        "calculate_f1_score(answer_file_roles, 'roles_based_level_num', 'Final AI Level')\n",
        "calculate_cohens_kappa(answer_file_roles, 'roles_based_level_num', 'Final AI Level')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCf52mxwPvVq"
      },
      "source": [
        "## Debate and Decide Toxicity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bw6lMRHQfHb"
      },
      "outputs": [],
      "source": [
        "!pip install convokit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOSXp-eqPuU0"
      },
      "outputs": [],
      "source": [
        "from convokit import Corpus, download\n",
        "corpus = Corpus(filename=download(\"conversations-gone-awry-corpus\"))\n",
        "# corpus = Corpus(filename=download(\"conversations-gone-awry-cmv-corpus\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rQbQCX7KxSy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n",
        "from openai import OpenAI\n",
        "from convokit import Corpus, download\n",
        "\n",
        "\n",
        "def clean_json(text):\n",
        "    text = text.strip()\n",
        "    if text.startswith(\"```json\"):\n",
        "        text = text[7:]\n",
        "    if text.endswith(\"```\"):\n",
        "        text = text[:-3]\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_first_two_utterances(convo):\n",
        "    utts = convo.get_chronological_utterance_list()\n",
        "    if len(utts) >= 2:\n",
        "        return utts[0].text.strip(), utts[1].text.strip()\n",
        "    return None, None\n",
        "\n",
        "\n",
        "def build_cga_wiki_dataset(csv_path=\"cga_wiki_pairs.csv\", jsonl_path=\"cga_wiki_pairs.jsonl\"):\n",
        "    corpus = Corpus(filename=download(\"conversations-gone-awry-corpus\"))\n",
        "    data = []\n",
        "\n",
        "    for convo in corpus.iter_conversations():\n",
        "        if convo.meta.get(\"conversation_has_personal_attack\", False):\n",
        "            pair_id = convo.meta.get(\"pair_id\")\n",
        "            if not pair_id or pair_id not in corpus.conversations:\n",
        "                continue\n",
        "            partner = corpus.get_conversation(pair_id)\n",
        "\n",
        "            utt1, utt2 = get_first_two_utterances(convo)\n",
        "            if utt1 and utt2:\n",
        "                data.append({\"conversation_id\": convo.id, \"text_pair\": f\"{utt1}\\n{utt2}\", \"label\": 1})\n",
        "\n",
        "            utt1, utt2 = get_first_two_utterances(partner)\n",
        "            if utt1 and utt2:\n",
        "                data.append({\"conversation_id\": partner.id, \"text_pair\": f\"{utt1}\\n{utt2}\", \"label\": 0})\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    df.to_json(jsonl_path, orient=\"records\", lines=True)\n",
        "    print(f\"Saved {len(df)} examples to CSV and JSONL.\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def label_toxicity(cga_df, output_file, api_key, n_samples=10):\n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "    initial_labels = []\n",
        "    ai_labels = []\n",
        "    final_rationales = []\n",
        "    pro_view_args = []\n",
        "    anti_view_args = []\n",
        "    anti_view_suggestions = []\n",
        "\n",
        "    for i in tqdm(range(n_samples)):\n",
        "        thread_text = cga_df['text_pair'].iloc[i]\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You will be provided with the first two messages from a Wikipedia talk page discussion.\n",
        "        Based on these messages alone, predict whether the conversation is likely to turn toxic later on (i.e., contain a personal attack).\n",
        "\n",
        "        Respond only with 'Yes' or 'No'. Do not include any explanation or punctuation.\n",
        "\n",
        "        Messages:\n",
        "        {thread_text}\n",
        "        \"\"\"\n",
        "\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"developer\", \"content\": \"You are a Wikipedia moderator trained to spot early signs of toxicity.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        initial_label = completion.choices[0].message.content.strip()\n",
        "        initial_numeric = 1 if initial_label == \"Yes\" else 0\n",
        "\n",
        "        final_label, rationale, pro_arg, anti_arg, anti_suggest = debate_and_decide_toxicity(\n",
        "            thread_text, initial_label, prompt, client\n",
        "        )\n",
        "        final_numeric = 1 if final_label == \"Yes\" else 0\n",
        "\n",
        "        initial_labels.append(initial_numeric)\n",
        "        ai_labels.append(final_numeric)\n",
        "        final_rationales.append(rationale)\n",
        "        pro_view_args.append(pro_arg)\n",
        "        anti_view_args.append(anti_arg)\n",
        "        anti_view_suggestions.append(anti_suggest)\n",
        "\n",
        "    cga_df = cga_df.iloc[:n_samples].copy()\n",
        "    cga_df['initial_ai_label'] = initial_labels\n",
        "    cga_df['debated_ai_label'] = ai_labels\n",
        "    cga_df['debated_ai_rationale'] = final_rationales\n",
        "    cga_df['pro_view_argument'] = pro_view_args\n",
        "    cga_df['anti_view_argument'] = anti_view_args\n",
        "    cga_df['anti_view_suggested_label'] = anti_view_suggestions\n",
        "\n",
        "    save_path = os.path.join(drive_folder, output_file)\n",
        "    output_df.to_csv(save_path, index=False)\n",
        "    return cga_df\n",
        "\n",
        "\n",
        "\n",
        "def debate_and_decide_toxicity(thread_text, initial_label, original_prompt, client):\n",
        "    # Pro View\n",
        "    pro_prompt = f\"\"\"\n",
        "    Given the following conversation:\n",
        "    {thread_text}\n",
        "\n",
        "    And an initial classification: {initial_label}\n",
        "\n",
        "    Argue in support of the decision to classify this thread as '{initial_label}'.\n",
        "    \"\"\"\n",
        "    pro_completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a Wikipedia moderator arguing in favor of the classification.\"},\n",
        "            {\"role\": \"user\", \"content\": pro_prompt}\n",
        "        ]\n",
        "    )\n",
        "    pro_argument = pro_completion.choices[0].message.content.strip()\n",
        "\n",
        "    # Anti View\n",
        "    anti_prompt = f\"\"\"\n",
        "    Given the following conversation:\n",
        "    {thread_text}\n",
        "\n",
        "    And an initial classification: {initial_label}\n",
        "\n",
        "    Argue why this classification might be incorrect. Suggest either 'Yes' or 'No' as a better label and explain why.\n",
        "\n",
        "    Respond in JSON format:\n",
        "    {{\n",
        "        \"Suggested_Label\": \"Yes\" or \"No\",\n",
        "        \"Argument\": \"Your reasoning here\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "    anti_completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a Wikipedia moderator reviewing a peer's classification.\"},\n",
        "            {\"role\": \"user\", \"content\": anti_prompt}\n",
        "        ]\n",
        "    )\n",
        "    try:\n",
        "        anti_data = json.loads(clean_json(anti_completion.choices[0].message.content.strip()))\n",
        "        anti_suggested = anti_data.get(\"Suggested_Label\", \"No\")\n",
        "        anti_argument = anti_data.get(\"Argument\", \"\")\n",
        "    except:\n",
        "        anti_suggested = \"No\"\n",
        "        anti_argument = \"Could not parse anti-view.\"\n",
        "\n",
        "    # Decider\n",
        "    decider_prompt = f\"\"\"\n",
        "    Conversation:\n",
        "    {thread_text}\n",
        "\n",
        "    Initial Classification: {initial_label}\n",
        "    Support Argument: {pro_argument}\n",
        "    Opposition Argument: {anti_argument}\n",
        "    Suggested Alternative: {anti_suggested}\n",
        "\n",
        "    Decide on the best final label: 'Yes' or 'No'.\n",
        "\n",
        "    Respond in JSON format:\n",
        "    {{\n",
        "        \"Final_Label\": \"Yes\" or \"No\",\n",
        "        \"Rationale\": \"Why this final label is best\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "    decider_completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a neutral moderator deciding between two arguments.\"},\n",
        "            {\"role\": \"user\", \"content\": decider_prompt}\n",
        "        ]\n",
        "    )\n",
        "    try:\n",
        "        decider_data = json.loads(clean_json(decider_completion.choices[0].message.content.strip()))\n",
        "        final_label = decider_data.get(\"Final_Label\", \"No\")\n",
        "        rationale = decider_data.get(\"Rationale\", \"\")\n",
        "    except:\n",
        "        final_label = \"No\"\n",
        "        rationale = \"Could not parse rationale.\"\n",
        "\n",
        "    return final_label, rationale, pro_argument, anti_argument, anti_suggested\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_P8waF1wK1Ej"
      },
      "outputs": [],
      "source": [
        "\n",
        "csv_out = \"debated_results_toxicity.csv\"\n",
        "\n",
        "# 1. Generate Data\n",
        "df = build_cga_wiki_dataset()\n",
        "\n",
        "# 2. Label Sample of 10\n",
        "labeled_df = label_toxicity(df, csv_out, API_KEY, n_samples=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B9AeiJC7Jkc"
      },
      "outputs": [],
      "source": [
        "def calculate_f1_score_debated(ai_csv_path,  ground_truth_csv_path='sampled_data.csv'):\n",
        "  ai_answers = pd.read_csv(ai_csv_path)['debated_ai_label']\n",
        "  labels = pd.read_csv(ai_csv_path)['label']\n",
        "  f1 = f1_score(labels, ai_answers, average='macro')\n",
        "  print(f\"F1 Score (Debated AI): {f1:.2f}\")\n",
        "\n",
        "def calculate_f1_score_initial(csv_path, ground_truth_csv_path='sampled_data.csv'):\n",
        "  ai_answers = pd.read_csv(csv_path)['initial_ai_label']\n",
        "  labels = pd.read_csv(csv_path)['label']\n",
        "  f1 = f1_score(labels, ai_answers, average='macro')\n",
        "  print(f\"F1 Score (Initial AI): {f1:.2f}\")\n",
        "\n",
        "def calculate_f1_score_verifier(csv_path, ground_truth_csv_path='sampled_data.csv'):\n",
        "    ai_answers = pd.read_csv(csv_path)['debated_ai_label']\n",
        "    labels = pd.read_csv(csv_path)['label']\n",
        "    f1 = f1_score(labels, ai_answers, average='macro')\n",
        "    print(f\"F1 Score (Verifier Suggestion): {f1:.2f}\")\n",
        "\n",
        "def calculate_cohens_kappa_debated(csv1, ground_truth_csv_path='sampled_data.csv'):\n",
        "    df1 = pd.read_csv(csv1)\n",
        "    ratings1 = df1['debated_ai_label']\n",
        "    ratings2 = df1['label']\n",
        "    normal_kappa = cohen_kappa_score(ratings1, ratings2)\n",
        "    weighted_kappa = cohen_kappa_score(ratings1, ratings2, weights='quadratic')\n",
        "    print(f\"Cohen's Kappa (Debated AI): {normal_kappa:.2f}\\nWeighted Cohen's Kappa (Debated AI): {weighted_kappa:.2f}\")\n",
        "\n",
        "def calculate_cohens_kappa_initial(csv_path, ground_truth_csv_path='sampled_data.csv'):\n",
        "    df1 = pd.read_csv(csv_path)\n",
        "    ratings1 = df1['initial_ai_label']\n",
        "    ratings2 = df1['label']\n",
        "    normal_kappa = cohen_kappa_score(ratings1, ratings2)\n",
        "    weighted_kappa = cohen_kappa_score(ratings1, ratings2, weights='quadratic')\n",
        "    print(f\"Cohen's Kappa (Initial AI): {normal_kappa:.2f}\\nWeighted Cohen's Kappa (Initial AI): {weighted_kappa:.2f}\")\n",
        "\n",
        "def calculate_cohens_kappa_verifier(csv_path, ground_truth_csv_path='sampled_data.csv'):\n",
        "    df1 = pd.read_csv(csv_path)\n",
        "    ratings1 = df1['debated_ai_label']\n",
        "    ratings2 = df1['label']\n",
        "    normal_kappa = cohen_kappa_score(ratings1, ratings2)\n",
        "    weighted_kappa = cohen_kappa_score(ratings1, ratings2, weights='quadratic')\n",
        "    print(f\"Cohen's Kappa (Verifier Suggestion): {normal_kappa:.2f}\")\n",
        "    print(f\"Weighted Cohen's Kappa (Verifier Suggestion): {weighted_kappa:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDJDoMUxoCxD"
      },
      "outputs": [],
      "source": [
        "# 3. Evaluate\n",
        "print(\"Metrics for Initial AI Response:\")\n",
        "calculate_f1_score_initial(csv_out)\n",
        "calculate_cohens_kappa_initial(csv_out)\n",
        "\n",
        "print(\"\\nMetrics for Debated AI Response:\")\n",
        "calculate_f1_score_debated(csv_out)\n",
        "calculate_cohens_kappa_debated(csv_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME7asSMhg8HW"
      },
      "outputs": [],
      "source": [
        "def label_toxicity(cga_df, output_file, api_key, n_samples=10):\n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "    initial_labels = []\n",
        "    ai_labels = []\n",
        "    final_rationales = []\n",
        "    pro_view_args = []\n",
        "    anti_view_args = []\n",
        "    anti_view_suggestions = []\n",
        "\n",
        "    for i in tqdm(range(n_samples)):\n",
        "        thread_text = cga_df['text_pair'].iloc[i]\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You will be provided with the first two messages from a Wikipedia talk page discussion.\n",
        "        Based on these messages alone, predict whether the conversation is likely to turn toxic later on (i.e., contain a personal attack).\n",
        "\n",
        "        Respond only with 'Yes' or 'No'. Do not include any explanation or punctuation.\n",
        "\n",
        "        Messages:\n",
        "        {thread_text}\n",
        "        \"\"\"\n",
        "\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"developer\", \"content\": \"You are a Wikipedia moderator trained to spot early signs of toxicity.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        initial_label = completion.choices[0].message.content.strip()\n",
        "        initial_numeric = 1 if initial_label == \"Yes\" else 0\n",
        "\n",
        "        final_label, rationale, verifier_justification, agree, suggested_label = verifier_and_decide_toxicity(\n",
        "            thread_text, initial_label, prompt, client\n",
        "        )\n",
        "        final_numeric = 1 if final_label == \"Yes\" else 0\n",
        "\n",
        "        initial_labels.append(initial_numeric)\n",
        "        ai_labels.append(final_numeric)\n",
        "        final_rationales.append(rationale)\n",
        "        pro_view_args.append(verifier_justification)\n",
        "        anti_view_args.append(agree)\n",
        "        anti_view_suggestions.append(suggested_label)\n",
        "\n",
        "    cga_df = cga_df.iloc[:n_samples].copy()\n",
        "    cga_df['initial_ai_label'] = initial_labels\n",
        "    cga_df['debated_ai_label'] = ai_labels\n",
        "    cga_df['debated_ai_rationale'] = final_rationales\n",
        "    cga_df['verifier_justification'] = verifier_justification\n",
        "    cga_df['verifier_agreed'] = agree\n",
        "    cga_df['verifier_suggested_label'] = suggested_label\n",
        "\n",
        "    save_path = os.path.join(drive_folder, output_file)\n",
        "    output_df.to_csv(save_path, index=False)\n",
        "    return cga_df\n",
        "\n",
        "def verifier_and_decide_toxicity(thread_text, initial_label, original_prompt, client):\n",
        "    # Verifier Agent\n",
        "    verifier_prompt = f\"\"\"\n",
        "    You are a verifier evaluating whether the initial classification is justified.\n",
        "\n",
        "    Initial Classification: '{initial_label}'\n",
        "    Conversation:\n",
        "    {thread_text}\n",
        "\n",
        "    Determine if the initial label is valid based on the conversation context. If you disagree, suggest a better label.\n",
        "\n",
        "    Respond in JSON:\n",
        "    {{\n",
        "        \"Agree\": true or false,\n",
        "        \"Suggested_Label\": \"Yes\" or \"No\",\n",
        "        \"Justification\": \"...\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    verifier_completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a Wikipedia moderator acting as a verifier of classification decisions.\"},\n",
        "            {\"role\": \"user\", \"content\": verifier_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    verifier_raw = verifier_completion.choices[0].message.content.strip()\n",
        "    try:\n",
        "        verifier_data = json.loads(clean_json(verifier_raw))\n",
        "        agree = verifier_data.get(\"Agree\", False)\n",
        "        suggested_label = verifier_data.get(\"Suggested_Label\", \"No\")\n",
        "        justification = verifier_data.get(\"Justification\", \"\")\n",
        "    except:\n",
        "        agree = False\n",
        "        suggested_label = \"No\"\n",
        "        justification = \"Could not parse verifier response.\"\n",
        "\n",
        "    # Decider Agent\n",
        "    decider_prompt = f\"\"\"\n",
        "    You are a neutral evaluator using the task rubric to make a final judgment.\n",
        "\n",
        "    Initial Classification: '{initial_label}'\n",
        "    Conversation:\n",
        "    {thread_text}\n",
        "\n",
        "    Verifier response:\n",
        "    Agree = {agree}\n",
        "    Suggested_Label = {suggested_label}\n",
        "    Justification = {justification}\n",
        "\n",
        "    Decide whether to stick with the initial label or use the verifier's alternative. Justify your answer.\n",
        "\n",
        "    Respond in JSON:\n",
        "    {{\n",
        "        \"Final_Label\": \"Yes\" or \"No\",\n",
        "        \"Rationale\": \"Brief reason for your decision\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    decider_completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a neutral moderator deciding whether to change the classification.\"},\n",
        "            {\"role\": \"user\", \"content\": decider_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        decider_data = json.loads(clean_json(decider_completion.choices[0].message.content.strip()))\n",
        "        final_label = decider_data.get(\"Final_Label\", \"No\")\n",
        "        rationale = decider_data.get(\"Rationale\", \"\")\n",
        "    except:\n",
        "        final_label = \"No\"\n",
        "        rationale = \"Could not parse rationale.\"\n",
        "\n",
        "    return final_label, rationale, justification, agree, suggested_label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6B44NTUjU-n"
      },
      "outputs": [],
      "source": [
        "csv_out = \"verified_results_toxicity.csv\"\n",
        "\n",
        "# 1. Generate Data\n",
        "df = build_cga_wiki_dataset()\n",
        "\n",
        "# 2. Label Sample of 10\n",
        "labeled_df = label_toxicity(df, csv_out, API_KEY, n_samples=1000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5zDtgz6oE5w"
      },
      "outputs": [],
      "source": [
        "# 3. Evaluate\n",
        "print(\"Metrics for Initial AI Response:\")\n",
        "calculate_f1_score_initial(csv_out)\n",
        "calculate_cohens_kappa_initial(csv_out)\n",
        "\n",
        "print(\"\\nMetrics for Verifier AI Response:\")\n",
        "calculate_f1_score_verifier(csv_out)\n",
        "calculate_cohens_kappa_verifier(csv_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyCXJQYGUIwR"
      },
      "source": [
        "roles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVhZ2znsUJfT"
      },
      "outputs": [],
      "source": [
        "def social_scientist_roles_toxicity(thread_text, initial_label, client):\n",
        "    # Social Scientist 1: Empirical analyst\n",
        "    analyst_prompt = f\"\"\"\n",
        "You are a social scientist studying online disinhibition and the early signs of toxicity in Wikipedia discussions.\n",
        "\n",
        "Conversation:\n",
        "{thread_text}\n",
        "\n",
        "The current prediction is: '{initial_label}' — this means the thread *is* (Yes) or *is not* (No) likely to become toxic.\n",
        "\n",
        "Do you agree with this prediction based on sociolinguistic cues (like escalating tone, sarcasm, or confrontation)? If not, suggest a corrected label.\n",
        "\n",
        "Respond in JSON:\n",
        "{{\n",
        "  \"Agree\": true or false,\n",
        "  \"Suggested_Label\": \"Yes\" or \"No\",\n",
        "  \"Justification\": \"Explain based on observed early cues\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    analyst_response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a sociolinguist trained in online behavior analysis.\"},\n",
        "            {\"role\": \"user\", \"content\": analyst_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        data = json.loads(clean_json(analyst_response.choices[0].message.content.strip()))\n",
        "        agree = data.get(\"Agree\", False)\n",
        "        suggested_label = data.get(\"Suggested_Label\", \"No\")\n",
        "        justification = data.get(\"Justification\", \"\")\n",
        "    except:\n",
        "        agree = False\n",
        "        suggested_label = \"No\"\n",
        "        justification = \"Parsing error: unable to extract rationale.\"\n",
        "\n",
        "    # Final Decision: Group Judgment\n",
        "    judge_prompt = f\"\"\"\n",
        "You are summarizing a decision made by a panel of social scientists reviewing the following discussion:\n",
        "\n",
        "{thread_text}\n",
        "\n",
        "The original model classified the thread as: '{initial_label}'.\n",
        "\n",
        "Panel member response:\n",
        "- Agreed with model? {agree}\n",
        "- Suggested_Label: {suggested_label}\n",
        "- Rationale: {justification}\n",
        "\n",
        "Based on this input, choose the best final label: 'Yes' (likely to become toxic later) or 'No' (unlikely to become toxic).\n",
        "\n",
        "Respond in JSON:\n",
        "{{\n",
        "  \"Final_Label\": \"Yes\" or \"No\",\n",
        "  \"Rationale\": \"Justify the final label based on social science reasoning\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    judge_response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are synthesizing feedback from a panel of researchers on toxicity prediction.\"},\n",
        "            {\"role\": \"user\", \"content\": judge_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        final_data = json.loads(clean_json(judge_response.choices[0].message.content.strip()))\n",
        "        final_label = final_data.get(\"Final_Label\", \"No\")\n",
        "        rationale = final_data.get(\"Rationale\", \"\")\n",
        "    except:\n",
        "        final_label = \"No\"\n",
        "        rationale = \"Parsing error in final decision.\"\n",
        "\n",
        "    return final_label, rationale, justification, agree, suggested_label\n",
        "\n",
        "def label_toxicity(cga_df, output_file, api_key, n_samples=10):\n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "    initial_labels = []\n",
        "    ai_labels = []\n",
        "    final_rationales = []\n",
        "    pro_view_args = []\n",
        "    anti_view_args = []\n",
        "    anti_view_suggestions = []\n",
        "\n",
        "    for i in tqdm(range(n_samples)):\n",
        "        thread_text = cga_df['text_pair'].iloc[i]\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You will be provided with the first two messages from a Wikipedia talk page discussion.\n",
        "        Based on these messages alone, predict whether the conversation is likely to turn toxic later on (i.e., contain a personal attack).\n",
        "\n",
        "        Respond only with 'Yes' or 'No'. Do not include any explanation or punctuation.\n",
        "\n",
        "        Messages:\n",
        "        {thread_text}\n",
        "        \"\"\"\n",
        "\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"developer\", \"content\": \"You are a Wikipedia moderator trained to spot early signs of toxicity.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        initial_label = completion.choices[0].message.content.strip()\n",
        "        initial_numeric = 1 if initial_label == \"Yes\" else 0\n",
        "\n",
        "        final_label, rationale, justification, agree, suggested_label = social_scientist_roles_toxicity(\n",
        "            thread_text, initial_label, client\n",
        "        )\n",
        "        final_numeric = 1 if final_label == \"Yes\" else 0\n",
        "\n",
        "        initial_labels.append(initial_numeric)\n",
        "        ai_labels.append(final_numeric)\n",
        "        final_rationales.append(rationale)\n",
        "        pro_view_args.append(justification)\n",
        "        anti_view_args.append(agree)\n",
        "        anti_view_suggestions.append(suggested_label)\n",
        "\n",
        "    cga_df = cga_df.iloc[:n_samples].copy()\n",
        "    cga_df['initial_ai_label'] = initial_labels\n",
        "    cga_df['debated_ai_label'] = ai_labels\n",
        "    cga_df['debated_ai_rationale'] = final_rationales\n",
        "    cga_df['panel_justification'] = pro_view_args\n",
        "    cga_df['panel_agreement'] = anti_view_args\n",
        "    cga_df['panel_suggested_label'] = anti_view_suggestions\n",
        "\n",
        "\n",
        "    cga_df.to_csv(output_file, index=False)\n",
        "    return cga_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOLzVAsaWCt0"
      },
      "outputs": [],
      "source": [
        "def calculate_f1_score_roles(csv_path, ground_truth_csv_path='sampled_data.csv'):\n",
        "    ai_answers = pd.read_csv(csv_path)['debated_ai_label']\n",
        "    true_labels = pd.read_csv(csv_path)['label']\n",
        "    f1 = f1_score(true_labels, ai_answers)\n",
        "    print(f\"F1 Score (Roles AI): {f1:.2f}\")\n",
        "\n",
        "def calculate_cohens_kappa_roles(csv_path, ground_truth_csv_path='sampled_data.csv'):\n",
        "    ai_answers = pd.read_csv(csv_path)['debated_ai_label']\n",
        "    true_labels = pd.read_csv(csv_path)['label']\n",
        "    kappa = cohen_kappa_score(true_labels, ai_answers)\n",
        "    print(f\"Cohen's Kappa (Roles AI): {kappa:.2f}\")\n",
        "    weighted_kappa = cohen_kappa_score(true_labels, ai_answers, weights='quadratic')\n",
        "    print(f\"Weighted Cohen's Kappa (Verifier Suggestion): {weighted_kappa:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_2HoXksULqM"
      },
      "outputs": [],
      "source": [
        "csv_out = \"roles_results_toxicity.csv\"\n",
        "\n",
        "# 1. Generate Data\n",
        "df = build_cga_wiki_dataset()\n",
        "\n",
        "# 2. Label Sample of 10\n",
        "labeled_df = label_toxicity(df, csv_out, API_KEY, n_samples=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4O6oqC5RUO86"
      },
      "outputs": [],
      "source": [
        "# 3. Evaluate\n",
        "print(\"Metrics for Initial AI Response:\")\n",
        "calculate_f1_score_initial(csv_out)\n",
        "calculate_cohens_kappa_initial(csv_out)\n",
        "\n",
        "print(\"\\nMetrics for Roles AI Response:\")\n",
        "calculate_f1_score_roles(csv_out)\n",
        "calculate_cohens_kappa_roles(csv_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkGarOTdCkUh"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVl7l7HQKx_U"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o22Jwry0CkUi"
      },
      "source": [
        "## Part 4: Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKIVs5ZkCkUi"
      },
      "source": [
        "### Model Performance Visualization"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
